{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86726dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental.pjit import pjit\n",
    "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
    "\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"Configuration for the Transformer model.\"\"\"\n",
    "    vocab_size: int = 10000\n",
    "    d_model: int = 256  # For a toy model, keep it small\n",
    "    num_heads: int = 4\n",
    "    num_layers: int = 3\n",
    "    d_ff: int = 1024  # Feed-forward hidden size\n",
    "    dropout_rate: float = 0.1\n",
    "    max_len: int = 128\n",
    "\n",
    "    # Training config\n",
    "    learning_rate: float = 1e-4\n",
    "    global_batch_size: int = 32\n",
    "    num_train_steps: int = 100\n",
    "\n",
    "# --- 2. Model Definition (Decoder-only Transformer) ---\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention module.\"\"\"\n",
    "    config: TransformerConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        head_dim = self.config.d_model // self.config.num_heads\n",
    "        \n",
    "        qkv = nn.Dense(features=self.config.d_model * 3, name=\"qkv_proj\")(x)\n",
    "        q, k, v = jnp.array_split(qkv, 3, axis=-1)\n",
    "\n",
    "        q = q.reshape(batch_size, seq_len, self.config.num_heads, head_dim).transpose(0, 2, 1, 3)\n",
    "        k = k.reshape(batch_size, seq_len, self.config.num_heads, head_dim).transpose(0, 2, 1, 3)\n",
    "        v = v.reshape(batch_size, seq_len, self.config.num_heads, head_dim).transpose(0, 2, 1, 3)\n",
    "\n",
    "        scores = jnp.einsum(\"bhid,bhjd->bhij\", q, k) / jnp.sqrt(head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = jnp.where(mask, scores, -1e9)\n",
    "\n",
    "        attn_weights = nn.softmax(scores, axis=-1)\n",
    "        attn_output = jnp.einsum(\"bhij,bhjd->bhid\", attn_weights, v)\n",
    "        attn_output = attn_output.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        output = nn.Dense(features=self.config.d_model, name=\"out_proj\")(attn_output)\n",
    "        return output\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    config: TransformerConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.config.d_ff)(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dense(features=self.config.d_model)(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block.\"\"\"\n",
    "    config: TransformerConfig\n",
    "    deterministic: bool\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, mask=None):\n",
    "        # Self-attention sublayer\n",
    "        attn_output = MultiHeadAttention(config=self.config, name=\"self_attention\")(x, mask)\n",
    "        attn_output = nn.Dropout(rate=self.config.dropout_rate)(attn_output, deterministic=self.deterministic)\n",
    "        x = nn.LayerNorm()(x + attn_output)\n",
    "\n",
    "        # Feed-forward sublayer\n",
    "        ffn_output = PositionwiseFeedForward(config=self.config, name=\"feed_forward\")(x)\n",
    "        ffn_output = nn.Dropout(rate=self.config.dropout_rate)(ffn_output, deterministic=self.deterministic)\n",
    "        x = nn.LayerNorm()(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Decoder-only Transformer model for language modeling.\"\"\"\n",
    "    config: TransformerConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, *, deterministic: bool):\n",
    "        _batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        tok_emb = nn.Embed(num_embeddings=self.config.vocab_size, features=self.config.d_model)(x)\n",
    "        pos_emb = self.param('pos_embedding', nn.initializers.zeros, (self.config.max_len, self.config.d_model))\n",
    "        pos_emb = pos_emb[:seq_len, :]\n",
    "        \n",
    "        x = tok_emb + pos_emb\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=deterministic)\n",
    "\n",
    "        # Causal mask for decoder\n",
    "        causal_mask = nn.make_causal_mask(x[:, :, 0])\n",
    "        \n",
    "        for i in range(self.config.num_layers):\n",
    "            x = TransformerBlock(config=self.config, deterministic=deterministic, name=f\"block_{i}\")(x, mask=causal_mask)\n",
    "\n",
    "        # Output logits\n",
    "        logits = nn.Dense(features=self.config.vocab_size, name=\"output_head\")(x)\n",
    "        return logits\n",
    "\n",
    "# --- 3. Training State and Step Functions ---\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    \"\"\"Custom TrainState to include dropout PRNG key.\"\"\"\n",
    "    dropout_key: jax.random.KeyArray\n",
    "\n",
    "def create_train_state(rng, model, optimizer, config):\n",
    "    \"\"\"Creates initial `TrainState`.\"\"\"\n",
    "    # We need a dummy input to initialize parameters\n",
    "    dummy_input = jnp.ones((1, config.max_len), dtype=jnp.int32)\n",
    "    params = model.init(rng, dummy_input, deterministic=True)['params']\n",
    "    return TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "        dropout_key=rng  # Initial dropout key\n",
    "    )\n",
    "\n",
    "def train_step(state, batch, config):\n",
    "    \"\"\"Performs a single training step.\"\"\"\n",
    "    # The loss function needs to be defined inside the train_step\n",
    "    # to capture the state and batch variables.\n",
    "    dropout_key, new_dropout_key = jax.random.split(state.dropout_key)\n",
    "\n",
    "    def loss_fn(params):\n",
    "        # The batch contains 'input_ids' and 'labels'\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Get logits from the model\n",
    "        logits = state.apply_fn(\n",
    "            {'params': params},\n",
    "            x=input_ids,\n",
    "            deterministic=False,\n",
    "            rngs={'dropout': dropout_key}\n",
    "        )\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits=logits, labels=labels\n",
    "        ).mean()\n",
    "        return loss\n",
    "\n",
    "    # Compute gradients\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    \n",
    "    # pjit will handle the gradient averaging across the 'data' axis of the mesh\n",
    "    # automatically because the loss is a mean over the global batch.\n",
    "    \n",
    "    # Update the state\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    # Update the dropout key\n",
    "    new_state = new_state.replace(dropout_key=new_dropout_key)\n",
    "    \n",
    "    return new_state, loss\n",
    "\n",
    "# --- 4. Main Training Script ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # This is required for multi-host training.\n",
    "    jax.distributed.initialize()\n",
    "\n",
    "    # Get process information\n",
    "    process_id = jax.process_index()\n",
    "    process_count = jax.process_count()\n",
    "    \n",
    "    if process_id == 0:\n",
    "        print(f\"Starting multi-host training on {process_count} processes.\")\n",
    "        print(f\"Total devices: {jax.device_count()}, Devices per process: {jax.local_device_count()}\")\n",
    "\n",
    "    # --- Distributed Setup ---\n",
    "    # Create a 1D mesh of all devices for data parallelism.\n",
    "    # The 'data' axis name is arbitrary but useful for annotation.\n",
    "    devices = np.array(jax.devices())\n",
    "    mesh = Mesh(devices, axis_names=('data',))\n",
    "    \n",
    "    if process_id == 0:\n",
    "        print(f\"Device mesh created: {mesh}\")\n",
    "\n",
    "    # Define sharding specifications using PartitionSpec (P).\n",
    "    P = PartitionSpec\n",
    "    \n",
    "    # Data will be sharded along the 'data' axis (batch dimension).\n",
    "    # (batch, seq_len) -> sharded on 'batch'\n",
    "    data_sharding = NamedSharding(mesh, P('data', None))\n",
    "    \n",
    "    # Model parameters will be replicated across all devices.\n",
    "    replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "    # --- Initialization ---\n",
    "    config = TransformerConfig()\n",
    "    model = Transformer(config)\n",
    "    optimizer = optax.adam(config.learning_rate)\n",
    "    \n",
    "    key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # To initialize the TrainState, we pjit a creation function.\n",
    "    # This ensures the state is created and sharded correctly from the beginning.\n",
    "    pjit_create_state = pjit(\n",
    "        create_train_state,\n",
    "        static_argnums=(1, 2, 3), # model, optimizer, config\n",
    "        in_shardings=(replicated_sharding,), # for the PRNG key\n",
    "        out_shardings=replicated_sharding   # The entire state is replicated\n",
    "    )\n",
    "    \n",
    "    # Create the sharded TrainState.\n",
    "    replicated_key = jax.device_put(key, replicated_sharding)\n",
    "    state = pjit_create_state(replicated_key, model, optimizer, config)\n",
    "\n",
    "    if process_id == 0:\n",
    "        print(\"TrainState created and replicated.\")\n",
    "\n",
    "    # --- Pjit the Training Step ---\n",
    "    pjit_train_step = pjit(\n",
    "        train_step,\n",
    "        static_argnums=(2,), # config\n",
    "        in_shardings=(replicated_sharding, data_sharding),\n",
    "        out_shardings=(replicated_sharding, replicated_sharding)\n",
    "    )\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for step in range(config.num_train_steps):\n",
    "        # --- Data Generation (on host, then sharded) ---\n",
    "        # In a real scenario, each host would load its own slice of data.\n",
    "        # Here, we generate the global batch on process 0 and then distribute it.\n",
    "        if process_id == 0:\n",
    "            input_ids = np.random.randint(0, config.vocab_size, \n",
    "                                          size=(config.global_batch_size, config.max_len), \n",
    "                                          dtype=np.int32)\n",
    "            labels = np.roll(input_ids, -1, axis=-1)\n",
    "            global_batch = {'input_ids': input_ids, 'labels': labels}\n",
    "        else:\n",
    "            # Other processes create empty containers. jax.device_put handles the scatter.\n",
    "            global_batch = {\n",
    "                'input_ids': np.zeros((config.global_batch_size, config.max_len), dtype=np.int32),\n",
    "                'labels': np.zeros((config.global_batch_size, config.max_len), dtype=np.int32)\n",
    "            }\n",
    "\n",
    "        # Distribute the batch from host CPU to devices with the specified sharding.\n",
    "        sharded_batch = jax.device_put(global_batch, data_sharding)\n",
    "\n",
    "        # Execute the training step\n",
    "        state, loss = pjit_train_step(state, sharded_batch, config)\n",
    "        \n",
    "        # Wait for the computation to finish before printing the loss.\n",
    "        jax.block_until_ready(loss)\n",
    "\n",
    "        if process_id == 0:\n",
    "            print(f\"Step {step+1}/{config.num_train_steps}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    if process_id == 0:\n",
    "        print(\"\\nTraining finished!\")\n",
    "\n",
    "def run_on_single_machine():\n",
    "    \"\"\"Helper to simulate a multi-host environment on a single machine.\"\"\"\n",
    "    try:\n",
    "        from jax.experimental.multihost_utils import run_on_hosts\n",
    "        print(\"Found multihost_utils. Spawning processes for demonstration.\")\n",
    "        run_on_hosts(main, n_hosts=jax.local_device_count())\n",
    "    except (ImportError, RuntimeError) as e:\n",
    "        print(f\"Could not use `run_on_hosts` ({e}). Running in a single process.\")\n",
    "        main()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # To simulate multi-host on a single machine with multiple GPUs/TPUs,\n",
    "    # you need to install the 'gloo' dependency for JAX: `pip install jax[gloo]`\n",
    "    if jax.local_device_count() > 1:\n",
    "        run_on_single_machine()\n",
    "    else:\n",
    "        print(\"Only one device found. Running in single-process, single-device mode.\")\n",
    "        main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
